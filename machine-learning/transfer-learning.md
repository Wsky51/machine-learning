## 迁移学习

### 一：概念

***啥是迁移学习***：拿别人已经训练好的模型，迁移到新的模型上来帮助模型训练。

***作用***：不同任务之间存在很多的内在相似性，可以微调现成模型来辅助自己新模型的开发，从而避免从零开始学习模型，加快学习效率。

> 我们大多数人在学习过程中是没有GPU资源，更别说TPU了，所以迁移学习对于初学者来说性价比还是很高的



迁移学习能够实用的原因还是在于，很多训练任务之间存在很多内在相似性，比如猫狗分类与猪牛分类，他们之间存在很多共同特征，此时如果把一个预训练好模型使用在另外一个有相似特征的任务上时，通过**mix**和**match**这些特征，就可以在新的数据集上有一个好的分类效果。

谷歌的**Inception-V3**就是基于**ImageNet**大体量数据训练而得的模型，虽然是针对于不同的图片分类，可是在底层图片特征的选择基本上都是一样的，故这也是迁移学习能够有非常好的效果的原因之一。此外，就**InceptionV3**模型而言，其内部大概有2500万参数，这对于没有大量数据集但对预测精确率要求较高的情况下，迁移学习是最符合我们的需求。

> 实际上，已经没有人会从零开始训练一个CNN网络了

**ImageNet**让我们有了众多的预训练模型，这些模型中包含有大量的已学习好的图片特征供我们使用，不同的模型对图片有不同的**理解**，而我们需要做的就是选择性的保留部分特征（layer的权重），有针对性的进行训练。

> 现在这个时代，我们从来不需要从头开始学习，都是站在巨人的肩膀上来解决新的问题



除了CV，迁移学习在NLP上也很好用。



### 二：迁移学习与传统机器学习有何不同

在一种数据上学习到的知识和经验，也是可以应用到另外一类数据上的，这也是从人的学习模式上借鉴过来的：比如一个人精通文学经典的学者和一个没读过书的文盲，让他们同时去阅读科技类的文章，很显然前者的效果肯定是更好一点。







参考文章：

[为什么迁移学习好用？](https://www.quora.com/Why-does-transfer-learning-works-Is-it-because-the-source-and-the-target-task-are-similar-Or-is-it-because-the-dataset-used-in-source-task-is-good-enough-that-its-representation-can-be-used-to-discover-patterns-from)





